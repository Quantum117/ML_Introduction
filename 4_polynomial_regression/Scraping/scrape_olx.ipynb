{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:42:37.450288Z",
     "start_time": "2025-12-10T05:42:37.256030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ],
   "id": "d74e3b271aba4e77",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:42:37.468175Z",
     "start_time": "2025-12-10T05:42:37.462747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_url = \"https://www.olx.uz\"\n",
    "url = 'https://www.olx.uz/oz/nedvizhimost/kvartiry/prodazha/tashkent/?currency=UYE'"
   ],
   "id": "56d62610903e4f9b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:42:37.489802Z",
     "start_time": "2025-12-10T05:42:37.484667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_href(url_): # get hrefs of all houses from url  one by one\n",
    "    page = requests.get(url_)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    houses1 =[]\n",
    "\n",
    "\n",
    "    # Find all divs with class 'something'\n",
    "    tags = soup.find_all(\"a\", class_=\"css-1tqlkj0\")\n",
    "    for a_tag in tags:\n",
    "      houses1.append(base_url + a_tag[\"href\"])\n",
    "    return houses1"
   ],
   "id": "679a3b93f4fb729c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:43:23.319278Z",
     "start_time": "2025-12-10T05:42:37.506165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#get first page\n",
    "houses = get_href(url)\n",
    "#get all remaining pages\n",
    "for i in range(2,25):\n",
    "    houses.extend(get_href(url+'&page='+str(i)))\n",
    "# remove duplicates\n",
    "houses = list(dict.fromkeys(houses))\n",
    "print(len(houses))"
   ],
   "id": "dbb54890e8bfe8aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:46:16.421782Z",
     "start_time": "2025-12-10T05:43:23.442866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def fetch_house_data(house_url, i=None):\n",
    "    try:\n",
    "        page = requests.get(house_url, timeout=10)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "        tags = soup.find(\"div\", class_=\"css-41yf00\")\n",
    "        if tags is None:\n",
    "            return None  # Skip this URL\n",
    "        # 1. scrape main house data\n",
    "        p_tags = tags.find_all(\"p\")\n",
    "        tag_content = {}\n",
    "        for tag in p_tags:\n",
    "            l = tag.text.split(':')\n",
    "            if len(l) > 1:\n",
    "                tag_content[l[0].strip()] = l[1].strip()\n",
    "        # 2.  find district location\n",
    "        loc_tag = soup.find(\"p\", class_=\"css-7wnksb\")\n",
    "        district = loc_tag.text.split(',')\n",
    "        if len(district) > 1:\n",
    "                tag_content['Tuman'] = district[1].split()[0].strip()\n",
    "        # 3. Get price\n",
    "        price_tag = soup.find(\"h3\", class_=\"css-fqcbii\")\n",
    "        price = price_tag.text.split()\n",
    "        tag_content['Narx'] = price[0]\n",
    "        # end\n",
    "        if i is not None:\n",
    "            print(f\"[{i}] Fetched: {house_url}\")\n",
    "        return tag_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {house_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_data_parallel(house_url_list, max_workers=12):\n",
    "    house_content = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_house_data, url, i): url for i, url in enumerate(house_url_list)}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                house_content.append(result)\n",
    "\n",
    "    return house_content\n",
    "\n",
    "data = get_data_parallel(houses, max_workers=12)\n"
   ],
   "id": "477c4da0d0971ce2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:46:16.595344Z",
     "start_time": "2025-12-10T05:46:16.588482Z"
    }
   },
   "cell_type": "code",
   "source": "features = ['Turarjoy turi','Xonalar soni','Umumiy maydon','Qavati', 'Uy qavatliligi','Mebelli','Tuman','Ta ºmiri','Narx']",
   "id": "77ebd8ec8ebf4e23",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:46:16.608899Z",
     "start_time": "2025-12-10T05:46:16.603353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "# save data to csv file\n",
    "with open('../housing_data.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=features)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow({k: row.get(k, '') for k in features})\n"
   ],
   "id": "fdb4fd8b1ea00af9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "data analysis",
   "id": "592e2bb1bcda0dc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:46:16.626162Z",
     "start_time": "2025-12-10T05:46:16.615927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "# read data to csv file\n",
    "with open('../housing_data.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f) # it gets headers and makes dictionaries  for every row\n",
    "    data = [row for row in reader]\n",
    "data"
   ],
   "id": "9e9d3eef7c6ca06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:46:16.643035Z",
     "start_time": "2025-12-10T05:46:16.637169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# distinct districts\n",
    "districts = set ([data[i]['Tuman'] for i in range(len(data))])\n",
    "dt ={}\n",
    "for item in districts:\n",
    "    dt[item]= 0\n",
    "for i in range(len(data)):\n",
    "    dt[data[i]['Tuman']] +=1\n",
    "print(dt)\n",
    "print(dt.keys())"
   ],
   "id": "577413be709b9838",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "dict_keys([])\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
